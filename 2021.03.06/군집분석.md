# ⭕ 군집분석

군집 분석(Cluster Analysis)은 각 개체의 유사성을 측정하여 유사성이 높은 대상 집단을 분류하고 군집에 속한 객체들의 유사성과 서로 다른 군집에 속한 개체간의 상이성을 규명하는 다변량 분석기법임. 별도의 반응변수(종속변수)가 요구되지 않기 때문에 비지도 학습(Unsupervised Learning)이며오로지 개체들 간의 유사성(similarity)에만 기초하여 군집을 형성함. 군집분석은 이상값 탐지에도 사용할 수 있으며, 다양한 분야에서 사용 중.

## ❗ 요인분석과 판별분석

요인분석: 유사한 변수를 함께 묶어주는 것이 목적   

판별분석: 사전에 집단이 나누어져 있는 자료를 통해 새로운 데이터를 기존의 집단에 할당하는 것이 목적   

요인과 판별의 차이점은 사전인가? 그렇지 않은가?에 초점을 두고 있으며 목적 자체가 다른 것에 집중하자.

## ❗ 거리

군집분석에서는 관측 데이터 간 유사성이나 근접성을 측정해 어느 군집으로 묶을 수 있는지 판단해야함. 따라서 이러한 유사성, 근접성을 계산하기 위해 거리라는 개념이 사용됨.

### ❓ 데이터가 연속형인 경우 

1. 유클리디안 거리

   - 데이터 간 유사성을 측정할 때 많이 사용. 하지만 변수들의 산포 정도가 전혀 고려되지 않음

2. 표준화

   - 해당변수의 표준편차로 척도 변환 후 유클리디안 거리를 사용하는 방법. 이때 척도의 차이, 분산의 차이로 인한 왜곡을 피할 수 있음

3. 마할라노비스

   - 통계적 개념이 포함된 거리. 표준화한 거리이며, 두 벡터 사이의 거리를 산포인 표준공분산으로 나눠주어 계산됨. 하지만 사전 지식 없이 표본의 표준공분산을 계산할 수 없으므로 사용하기 곤란함.

4. 체비셰프

   - 개체의 차이가 가장 큰 거리를 사용하는 방법

5. 맨하탄

   개체에서 개체의 최단 거리를 구하기 위해 고안된 거리. 차이의 합

6. 캔버라

   맨하탄에서 개체간의 차이에 개체의 합을 나누어 합한 거리

7. 민코우스키

   맨하탄과 유클리디안 거리를 한번에 표현한 공식

### ❓ 범주형 변수인 경우

1. 자카드 거리
   - 합집합에 교집합을 뺀 것으로 교집합의 개수가 많을수록 거리가 짧아짐.
2. 코사인 유사도
   - 두 개체의 백터 내적의 코사인 값을 이용하여 측정된 벡터간의 유사한 정도
   - 문서 간 유사도를 측정할 때 많이 이용함.(벡터의 크기는 유사성에 아무런 영향을 주지 않음)

# ⭕ 계층적 군집분석

계층적 군집분석은 n개의 군집으로 시작해 점차 군집의 수를 줄여나가는 방법. 군집을 형성하는 방법은 합병형과 분리형으로 나뉨.

## ❗ 계층적 군집분석의 종류

1. 최단연결
   - 거리행렬에서 최단거리를 거리로 계산하여 거리행렬을 수정함. 이때 군집화하여 진행.
2. 최장연결
   - 최단연결과는 반대로 거리를 계산할 때 최장거리를 계산하여 거리행렬을 수정.
3. 평균연결
   - 마찬가지의 맥락으로 평균을 계산하여 거리행렬을 수정.
4. 와드연결
   - 군집내 편차들의 제곱합을 고려한 방법.

## ❗ R에서 계층적 군집분석 진행하는 법

`dist()`함수와 `hclust()`함수로 수행할 수 있음

```R
dist(data, method)
# data: 군집화할 데이터
# method: 거리측정방법 
# 'euclidean', 'maximum', 'manhattan', 'canberra', 'binary', 'minkowski'
# dist함수는 계층적 군집분석 전에 각 개체의 거리를 구하는데 이용

hclusst(data, method)
# data: 군집화할 데이터
# method: 거리측정방법
# 'signle', 'complete', 'average', 'median', 'ward.D'
```

USArrests 데이터로 실습

```R
# dist 함수를 이용하여 거리를 구하기
US = USArrests
US_data = dist(US, 'euclidean')
US_data

# hclust함수를 이용하여 계층적 군집분석
US_single = hclust(US_data^2, method = 'single')
plot(US_single)
US_complete = hclust(US_data^2, method = 'complete')
US_average = hclust(US_data^2, method = 'average')

group = cutree(US_average, k = 6)
group
# 계층적 군집 결과 그룹 나누기와 덴드로그램 구분 짓기

plot(US_average)
rect.hclust(US_average, k = 5, border = 'red')
# k의 역할은 군집을 몇개로 나눌것인지 선택하느느 것
```

# ⭕ 비계층적 군집분석

n개의 개체를 k개의 군집으로 나눌 수 있는 모든 가능한 방법을 점검해 군집을 형성하는 것. 

## ❓ K-평균 군집분석(K-Means clustering)

주어진 데이터를 k개의 클러스터로 묶는 알고리즘으로, 각 클러스터와 거리 차이의 최소화 하는 방식으로 동작한다.

- 원하는 군집의 개수와 초기 값(seed)들을 정해 seed 중심으로 군집을 형성한다.
- 각 데이터를 거리가 가장 가까운 seed가 있는 군집으로 분류한다.
- 각 군집의 seed값을 다시 계산한다.

### ✔ 특징

- 거리 계산을 통해 군집화가 이루어지므로 연속형 변수에 활용이 가능하다.
- k개의 초기 중심값은 임의로 선택이 가능하며 가급적이면 멀리 떨어지는 것이 바람직하다.
- 초기 준심값을 임의로 선택할 때 일렬(위아래, 좌우)로 선택하면 군집이 혼합되지 않고 층으로 나누어질 수 있어 주의하여야 한다. 즉 초기 중심값에 따라 결과가 달라질 수 있다.
- 초기 중심으로부터의 오차 제곱합을 최소화하는 방향으로 군집이 형성되는 탐욕적(greedy) 알고리즘이므로 안정된 군집은 보장하나 최적이라는 보장은 없다.

### ✅ 장점

-  알고리즘이 단순하여 빠르게 수행되고 분석 방법 적용이 용이
- 계층적 군집분석에 비해 많은 양의 데이터를 다룰 수 있음(계층적 군집분석은 데이터 수에 따라 계산되는 양이 기하급수적으로 늘어남)
- 내부 구조에 대한 사전정보가 없어도 의미있는 자료구조를 찾을 수 있음.
- 다양한 형태의 데이터에 적용이 가능

### ❌ 단점

- 초기 군집의 수, 가중치와 거리를 정의하기 어려움
- 사전에 주어진 목적이 없으므로 결과 해석이 어려움
- 잡음이나 이상값의 영향을 많이 받음
- 블록한 형태가 아닌 군집이 존재할 경우 성능이 떨어짐
- 초기 군집 수 결정에 어려움이 있음

## ❗ R에서 비계층적 군집분석 진행하는 법

`kmeans()` 함수 이외에 `NbClust` 패키지의 `NbClust()` 함수로 최적의 k를 선정할 수 있음.

```R
kmeans(data, centers)
# data: 분석하고자 하는 데이터
# centers: 군집의 개수를 결정
NbClust(data, min.nc, max.nc, method)
# data: 분석할 데이터
# min.nc: 최소 군집의 수
# max.nc: 최대 군집의 수
# method: 군집분석 방법을 정함
# 'kmeans', 'median', 'single', 'complete', 'average'
```

Credit Data로 실습

```R
train_data = train[,-1]
credit_kmeans = kmeans(train_data, centers = 2)
credit_kmeans
kmeans_table = table(train$credit.rating, credit_kmeans$cluster)
kmeans_table
(kmeans_table[2,1] + kmeans_table[1,2]) / sum(kmeans_table)
[1] 0.6685714
```

kmeans 군집 분석을 실시하기 전에 종속변수를 제외한 데이터로 군집분석을 실시해 원래 종솝변수와 군집분석 결과의 정분류율을 확인하자.

분석 결과 583,117의 개체가 모인 군집으로 나누어짐. 그리고 전체 변동에서 군집 간 변동이 차지하는 비율인 (between_SS / total_SS)이 1에 가까울수록 군집이 잘 분류되었다고 판단하지만, 70.2%로 좋은 모델이라고 할 수 없음

정분류율은 0.6685로 좋은 성능을 갖진 않았음

`NbClust()`를 사용하여 최적의 군집 수 찾기

```R
library(NbClust)
nc = NbClust(train_data, min.nc = 2, max.nc = 15, method = 'kmeans')
# NbClust를 사용할 때 연산이 오래된다는 점을 고려해야함....
* According to the majority rule, the best number of clusters is  2 
```

최적 k는 2로 계산되었음. kmeans()함수를 사용하기 전에 NbClust()를 사용해 최적의 K를 찾고 분석하는 것이 필요함.

# ⭕ 혼합 분포 군집

모형 기반(model_base)의 군집 방법, 데이터가 k개의 모수적 모형(흔히 정규분포 또는 다변량 정규분포를 가정함)의 가중합으로 표현되는 모집단 모형으로부터 나왔다는 가정ㅇ하에서 모수와 함께 가중치를 자료로부터 추정하는 방법을 사용

k개의 각 모형은 군집을 의미하며, 각 데이터는 추정된 k개의 모형 중 어느 모형으로부터 나왔을 확률이 높은지에 따라 군집의 분류가 이루어진다.

흔히 혼합모형에서의 모수와 가중치의 추정에는 EM 알고리즘이 사용

## ❗ EM 알고리즘의 진행 과정

- 각 자료에 대해 z의 조건부분포로부터 조건부 기댓값을 구할 수 있다.
- 관측변수 X와 잠재변수 Z를 포함하는 (X, Z)에 대한 로그-가능도함수에 Z 대신 상수값인 Z의 조건부 기대값을 대입하면, 로그-가능도함수를 최대로 하는 모수를 쉽게 찾을 수 있다. (M-단계)  갱신된 모수 추정치에 대해 위 과정을 반복한다면 수렴하느느 값을 얻게 되고, 이는 최대 가능도 추정치로 사용될 수 있다.
- E-단계: 잠재변수 Z의 기대치 계산
- M-단계: 잠재변수 X의 기대치를 이용하여 파라미터를 추정

### ✔ 특징

- K 평균군집의 절차와 유사하지만 확률분포를 도입하여 군집을 수행한다.
- 군집을 몇 개의 모수로 표현할 수 있으며, 서로 다른 크기나 모양의 군집을 찾을 수 있다.
- EM 알고리즘을 이용한 모수 추정에서 데이터가 커지면 수렴에 시간이 걸릴 수 있다.
- 군집의 크기가 너무 작으면 추정의 정도가 떨어지거나 어려울 수 있다.
- K-평균군집과 같이 이상치 자료에 민감하므로 사전에 조치가 필요하다.

##  ❗ R에서 비계층적 군집분석 진행하는 법

R에서 혼합 분포 군집분석을 수행할 수 있는 패키지는 `mixtools`, `mclust`, `norlmix`, `HDclassif`, `EMcluster` 등이 있으며, 주로 `mclust` 패키지의 `Mclust()`함수를 통한 혼합 분포 군집분석을 수행한다.

```R
Mclust(data, G, ...)
# data : 분석하고자 하는 데이터(벡터, 매트릭스, 데이터 프레임 다 가능)
# 하지만 수치형 변수만 가능하니까
# as.numeric()함수를 사용하여 사전에 데이터를 강제 변환해야함
# G : BIC를 계산할 혼합분포 클러스터의 수를 지정, Default는 1:9
```

iris Data로 실습

```R
library(mclust)
mc = Mclust(iris[,1:4], G = 3)
summary(mc, parameters = T)
---------------------------------------------------- 
Gaussian finite mixture model fitted by EM algorithm 
---------------------------------------------------- 

Mclust VEV (ellipsoidal, equal shape) model with 3 components: 

 log-likelihood   n df       BIC       ICL
       -186.074 150 38 -562.5522 -566.4673

Clustering table:
 1  2  3 
50 45 55 

Mixing probabilities:
        1         2         3 
0.3333333 0.3005423 0.3661243 

Means:
              [,1]     [,2]     [,3]
Sepal.Length 5.006 5.915044 6.546807
Sepal.Width  3.428 2.777451 2.949613
Petal.Length 1.462 4.204002 5.482252
Petal.Width  0.246 1.298935 1.985523

Variances:
[,,1]
             Sepal.Length Sepal.Width Petal.Length Petal.Width
Sepal.Length   0.13320850  0.10938369  0.019191764 0.011585649
Sepal.Width    0.10938369  0.15495369  0.012096999 0.010010130
Petal.Length   0.01919176  0.01209700  0.028275400 0.005818274
Petal.Width    0.01158565  0.01001013  0.005818274 0.010695632
[,,2]
             Sepal.Length Sepal.Width Petal.Length Petal.Width
Sepal.Length   0.22572159  0.07613348   0.14689934  0.04335826
Sepal.Width    0.07613348  0.08024338   0.07372331  0.03435893
Petal.Length   0.14689934  0.07372331   0.16613979  0.04953078
Petal.Width    0.04335826  0.03435893   0.04953078  0.03338619
[,,3]
             Sepal.Length Sepal.Width Petal.Length Petal.Width
Sepal.Length   0.42943106  0.10784274   0.33452389  0.06538369
Sepal.Width    0.10784274  0.11596343   0.08905176  0.06134034
Petal.Length   0.33452389  0.08905176   0.36422115  0.08706895
Petal.Width    0.06538369  0.06134034   0.08706895  0.08663823
```

`Mclust()`함수를 사용하여 클러스터의 수를 3으로 정하였으며, summary() 함수의 parameter 인자를 True로 하여 혼합분포의 모수추정치와 함께 각 군집별 해당 자료에 대한 요약 결과를 확인할 수 있음.

```R
plot.Mclust(mc)

Model-based clustering plots: 

1: BIC
2: classification
3: uncertainty
4: density

선택: 2

mc$classification
  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2
 [54] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 3 2 3 2 2 2 2 3 2 2 2 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3
[107] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
```

plot.Mclust() 함수를 사용하여 다양한 방식으로 군집 결과를 시각화할 수 있으며, $classification 인자를 통해 각 개제가 어느 그룹으로 분류되었는지를 확인할 수 있음.