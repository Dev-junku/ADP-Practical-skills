# ⭕ 텍스트마이닝

텍스트마이닝은 텍스트로부터 고품질의 정보를 도출하는 분석 방법으로, 일력된 텍스트를 구조화해 그 데이터에서 패턴을 도출한 후 결과를 평가 및 해석하는 일련의 과정을 의미. 주로 구조화된 정형 데이터 속에서 정보나 패턴을 발견하는 데이터 마이닝과는 달리 텍스트 마이닝은 인터넷 데이터, 소셜 미디어 데이터 등과 같은 자연어로 구성된 비정형 데이터 속에서 정보나 관계를 발견하는 분석 기법임.

단어들 간의 관계를 이용해 감성분석, 워드클라우드 분석 등을 수행한 후 이 정보를 클러스터링, 분류, 사회연결망 분석 등에 활용.

## ❓ 데이터 전처리

수집된 텍스트 데이터에 문장 부호나 의미 없는 숫자와 단어, url 등과 같이 분석을 하는데 유의하게 사용되지 않은 부분이 있음. 이거 제거하는게 일임.

데이터 전처리를 통해 필요한 데이터만 추출하기 위해 사용되지 않는 부분을 제거하거나 변형하여 분석에 용이하게 텍스트를 가공하는 과정을 진행.

### ✅ `tm` 패키지

- `tm`패키지는 문서를 관리하는 기본 구조인 `Corpus`를 생성하고 `tm_map()` 함수를 통해 데이터를 전처리 및 가공함.

- `Corpus`와 `VCorpus`중 `VCorpus`에서 에러가 적게 나타나므로 주로 `VCorpus`를 이용

#### 1️⃣ `Corpus`란?

- `Corpus`는 데이터 마이닝 절차 중 데이터의 정제, 통합, 선택, 변환의 과정을 거친 구조화된 단계로 더 이상 추가적인 절차 없이 데이터마이닝 알고리즘 실험에 활용될 수 있는 상태
- R 프로그램의 텍스트 마이닝 패키지인 `tm`에서 문서를 관리하는 기본 구조이며, 텍스트 문서들의 집합을 의미.
- `VCorpus`: 문서를 `Corpus class`로 만들어 주는 함수로, 결과는 메모리에 저장되어 현재 구동중이 R메모리에서만 유지됨. ❗❗

#### 2️⃣ `tm `패키지를 활용한 `Corpus` 만들기

- 텍스트 마이닝을 수행하기 전에 `tm` 패키지를 활용해 `Corpus`를 만들고 생성된 `Corpus`를 전처리, 분석에 활용해야함.
- 텍스트 데이터를 문서로 만들기 위해 `VectorSource()`함수를 사용하고 문서로 완성된 데이터를 `VCoupus()` 함수를 이용하여 Corpus를 만듬

```R
VectorSource(text)
# text: 문서로 변경하고자 하는 텍스트 데이터
```

```R
VCorpus(data)
# data: VectorSource()함수를 실행한 데이터
```

실습!

```R
install.packages('tm')
library(tm)

data(crude)
summary(crude)[1:6,]
    Length Class             Mode
127 2      PlainTextDocument list
144 2      PlainTextDocument list
191 2      PlainTextDocument list
194 2      PlainTextDocument list
211 2      PlainTextDocument list
236 2      PlainTextDocument list

inspect(crude[1])
<<VCorpus>>
Metadata:  corpus specific: 0, document level (indexed): 0
Content:  documents: 1

$`reut-00001.xml`
<<PlainTextDocument>>
Metadata:  15
Content:  chars: 527

crude[[1]]$content
[1] "Diamond Shamrock Corp said that\neffective today it had cut its contract prices for crude oil by\n1.50 dlrs a barrel.\n    The reduction brings its posted price for West Texas\nIntermediate to 16.00 dlrs a barrel, the copany said.\n    \"The price reduction today was made in the light of falling\noil product prices and a weak crude oil market,\" a company\nspokeswoman said.\n    Diamond is the latest in a line of U.S. oil companies that\nhave cut its contract, or posted, prices over the last two days\nciting weak oil markets.\n Reuter"

```

`crude` 데이터는 로이터 뉴스 기사 중 원유와 관련된 기사 20개가 저장된 데이터이다. `summary()`결과에서 `class`는 `TextDocument` 형태이면서 `list`형태로 저장되어 있음.

`inspect()`함수로 문서의 정보를 파악할 수 있음(연관분석에서와 같은 이치의 역할임). 문서의 내용은 `$content`를 붙여서 확인할 수 있음.

#### 3️⃣ `tm `패키지를 활용한 데이터 전처리

- 코퍼스(`Corpus`)로 변환된 데이터를 `tm_map()`함수를 활용하여 텍스트 전처리를 수행할 수 있음.
- 공백 제거, 문장부호 제거, 숫자 제거, 불용어 제거 등의 전처리를 통해 텍스트 마이닝을 수행하기 위한 데이터를 만들 수 있음 있음.

```R
tm_map(x, FUN)
# x: 코퍼스로 변환된 데이터
# FUN: 변환에 사용할 함수를 입력
# FUN에 들어갈 수 있는 종류
# tolower -> 소문자로 만들기
# stemDocument -> 어근만 남기기
# stripWhitespace -> 공백제거
# removePunctuation -> 문장부호제거
# removeNumbers -> 숫자제거
# removeWords, '단어' -> 해당 단어 지우기
# removeWords, stopwords('english') -> 불용어 지우기
# PlainTextDocument -> TextDocument로 변환
```

실습!

```R
# 뉴스 실습
news = readLines('뉴스기사.txt')
news_corpus = VCorpus(VectorSource(news))
news_corpus[[1]]$content

# 숫자, 문장부호, 공백 제거
clean_txt = function(txt) {
  txt = tm_map(txt, removeNumbers) # 숫자
  txt = tm_map(txt, removePunctuation) # 문장부호
  txt = tm_map(txt, stripWhitespace) # 공백
  return(txt)
}

clean_news = clean_txt(news_corpus)
clean_news[[1]]$content # ',·와 같은 문제는 제거되지 않음

txt2 = gsub('[[:punct:]]',"",clean_news[[1]]) # gsub함수는 엑셀 등에서의 찾아바꾸기의 기능을 수행
# 완벽히 ',·가 지워졌음을 확인
```

- 예제 텍스트 파일을 `readLines()`함수로 `R`프로그램에 `news`라는 변수에 저장. 텍스트 데이터를 `VectorSource()`함수를 통해 문서를 만들고, `VCorpus()`함수로 `Corpus` 형태로 변환.
- `Corpus`로 변환된 데이터는 리스트 형태이고 `$content`를 통해 내용을 확인할 수 있음. `Corpus`데이터를 전처리하기 위해 사용자 지정함수를 제작하여 데이터 전처리를 수행.
- `tm_map()`함수로 숫자, 문장부호, 공백 제거를 실시(`clean_txt()` 사용자 정의함수를 이용) 하지만 ',· 가 지워지지 않았음. 따라서 `gsub('[[:punct:]]', '', clean_news[[1]])`로 해당 문자를 지워줌 `[[:punct:]]`와 같은 용어로 전체 대체가 가능.

`gsub()`정리

```R
[[:punct:]] # 특수문자
[[:digit:]] # 숫자
[[A-z]] # 알파벳
[[:alnum:]] # 영문자/숫자
```

## ❓ 자연어 처리

자연어 처리는 기본적으로 형태소 분석을 하는 과정을 포함하고 있음. 문장의 품사를 구분하여 분석에 필요한 품사만 추축하여 활용할 수 있음.

R에서 한글 자연어 분석을 하기 위해 `KoNLP` 패키지를 이용한다. 25개의 함수가 들어있으며, 형태소 분섣 들의 자연어 처리 및 텍스트 마이닝을 수행할 수 있다.

### ✅ `KoNLP`패키지를 활용한 한글 처리

- `KoNLP` 패키지를 활용한 자연어 처리에 앞서 품사 등을 포함하고 있는 사전을 설정함. 주로 `KoNLP`에 `SejongDic`를 사전으로 등록하여 사용한다.
- 직접 사전에 단어를 추가하여 위해서 `buildDictionart()`함수를 사용하고 사용하고 `simplePos22()`함수를 통해 문장을 형태소로 분리하여 22개의 품사 태그를 달아 준다. `extraNoun()`함수는 문장에서 명사만 추출하고 결과를 확인할 수 있음.

함수 사용법

 KoNLP 패키지를 사용하기 위해서는 일단 java를 다운로드 받아야함.
 그리고 rJava, KoNLP 패키지를 차례로 다운로드 받아야함.
 일단 다운로드가 지원되는 버전이 생각보다 낮은 것도 문제...
 최신 버전에서 사용하는것은 지양... 저 같은 경우도 3.6.3 버전으로 시작했습니다.

```R
buildDictionary(ext_dic, data)
# ext_dic: 단어를 추가하고자 하는 사전을 선택. 'woorimalsam', 'sejong', 'insighter'
# data: 추가하고자 하는 단어와 품사가 들어간 data.frame 또는 txt파일
extraNoun(text)
# text: 명사를 추출하고자 하는 문장 또는 문서
SimplePoss22(text)
# text: 형태소 분석을 하고자 하는 문장 또는 문서
```

```R
library(rJava)
library(KoNLP)
useSejongDic() # 세종사전
sentence = '아버지가 방에 스르륵 들어가신다.'

extractNoun(sentence)
[1] "아버지" "방"     "스르륵"

buildDictionary(ext_dic = 'sejong', # 스르륵은 명사가 아님
                user_dic = data.frame(c('스르륵'),c('mag')))

extractNoun(sentence)
[1] "아버지" "방" 

SimplePos22(sentence)
$아버지가
[1] "아버지/NC+가/JC"

$방에
[1] "방/NC+에/JC"

$스르륵
[1] "스르륵/MA"

$들어가신다
[1] "들/PV+어/EC+가/PX+시/EP+ㄴ다/EF"

$.
[1] "./SF"
```

- 스르륵은 명사가 아니므로, `user_dic = data.frame(c('스르륵'),c('mag'))`를 통해  부사로 새롭게 등록해줌.

- NC는 명사, PV는 동사, PA는 형용사

- news 데이터에서 Corpus변환하지 않고 전처리 및 명사추출, 사전추가, 품사확인을 하고 형용사를 추출해보기!

```R
clean_txt2<-function(txt){
  txt<-removeNumbers(txt)          #숫자 제거
  txt<-removePunctuation(txt)      #문장부호 제거
  txt<-stripWhitespace(txt)        #공백제거
  txt<-gsub("[^[:alnum:]]"," ",txt) #영숫자, 문자를 제외한 것들을 " "으로 처리
  return(txt)
}

clean_news2 = clean_txt2(news)
Noun.news<-extractNoun(clean_news2)

Noun.news[5]
buildDictionary(ext_dic = 'sejong',
                user_dic = data.frame(c(read.table('clipboard'))))

extractNoun(clean_news2[5])
```

전처리를 마친 데이터를 `Corpus`로 변환하지 않고 데이터를 확인했을 때, '푸드테크', '스타트업' 등과 같은 복합명사가 분리되어 출력되는 것을 확인할 수 있음. 이러한 것도 하나도 익식할 수 있게 끔 사전을 구축하여 다시 `extracNoun()`함수를 실행

```R
install.packages('stringr')
library(stringr)
doc1 = paste(SimplePos22(clean_news2[[2]]))
doc1

doc2 = str_match(doc1, '([가-힣]+)/PA')
doc2

doc3 = doc2[,2]
doc3[!is.na(doc3)]
[1] "더하"   "빠르"   "새롭"   "빠르"   "빠르"   "이러하" "걸맞"  
```

- `stringr`은 R에서 문자열을 처리할 수 있는 패키지로, str_match함수로 문자열 증 특정 부분이 해당하는 데이터를 선별할 수 있음.

- `SimplePos22()`함수를 실행한 결과가 `리스트 형태로` 나타나므로 이를 `paste()`를 사용해 `character`형 벡터로 변형하여 doc1에 저장한다. `str_match` 함수를 활용해 품사 중 PA(형용사)인 데이터만 추출
- `doc2` 데이터에서 1열은 `PA`를 포함한 단어가 있는 열이며, 2열은 `PA`를 제외한 단어만 있는 열이 생성되고 `PA`가 없는 행은 `NA`로 채워짐. `doc2`의 데이터를 `doc3`에 저장하고 `is.na()`함수로 `NA`를 제외한 데이터만 추출함.

### ✅ Stemming

- 어간추출(`Stemming`)은 형태학적 분석을 단순화한 버전이라고 할 수 있음. 정해진 규칙만 보고 단어의 어미를 자르는 어림짐작의 작업이라고 할 수 있음. 즉, 공통 어간을 가지는 단어를 묶는 작업은 `Stemming`이라고 함.
- R에서는 `tm`패키지에서 `stemDocument()`함수를 통해 공통으로 들어가지 않은 부분을 제외하고 `stemCompletion()`함수를 통해 `stemming`된 단어와 완성을 위한 `dictionary`를 함께 넣으면 가장 기본적인 어휘로 완성해주는 역할을 함.

```R
stemDocument(text)
# text: 문자 또는 문서
stemDocument(text, dictionary)
# text: stemming이 완료된 문자 또는 문서
# dictionary: 어간 추출이 필요한 단어를 사전에 추가
```

실습!

```R
test = stemDocument(c('analyze', 'analyzed', 'analyzing'))
test
[1] "analyz" "analyz" "analyz" # 진짜 어간만 추출
completion = stemCompletion(test, dictionary = c('analyze', 'analysed','analyzing'))
completion

   analyz    analyz    analyz 
"analyze" "analyze" "analyze"
```

`stemDocument()`함수를 통해 앞 어간을 제외한 나머지 부분을 잘려 나가게 만들어 각 단어가 서로 다르지만 사실 모두 analyz-라는 어간을 가지므로 위와 같이 도출됨

`temCompletion()`함수를 통해 `analyz`로 `stemming` 되었던 단어들이 `dictionary`에 포함된 단어 중 가장 기본 어휘로 완성된 것을 확인할 수 있음. 가장 중요한 것은 `stemCompletion()`을 할 때는 단어의 완성을 위해 반드시 `dictionary`가 필요함(`vector`형식으로 입력).