clean_txt2 = function(txt) {
txt = removeNumbers(txt)
txt = removePunctuation(txt)
txt = stripWhitespace(txt)
txt = gsub('[^[:alnum:]]','',txt)
return(txt)
}
clean_news2 = clean_txt2(news)
Noun.news[5]
clean_news2[5]
extractNoun(clean_news2)
Noun.news<-extractNoun(clean.news2)
Noun.news<-extractNoun(clean_news2)
clean_txt2<-function(txt){
txt<-removeNumbers(txt)          #숫자 제거
txt<-removePunctuation(txt)      #문장부호 제거
txt<-stripWhitespace(txt)        #공백제거
txt<-gsub("[^[:alnum:]]"," ",txt) #영숫자, 문자를 제외한 것들을 " "으로 처리
return(txt)
}
clean_news2 = clean_txt2(news)
Noun.news<-extractNoun(clean_news2)
Noun.news[5]
buildDictionary(ext_dic = 'sejong',
user_dic = data.frame(c(read.table('clipboard'))))
extractNoun(clean_news2[5])
install.packages('stringr')
save.image("C:/Users/junkyu/Desktop/DATA ANALYSIS/모형 실습.RData")
install.packages('stringr')
install.packages("stringr")
library(stringr)
library(rJava)
library(KoNLP)
library(tm)
doc1 = paste(SimplePos22(clean_news2[[2]]))
library(stringr)
doc1 = paste(SimplePos22(clean_news2[[2]]))
doc1
doc2 = str_match(doc1, '([가-힣]+)/PA')
doc2
doc3 = doc2[,2]
doc[!is.na(doc3)]
doc3[!is.na(doc3)]
test = stemDocument(c('analyze', 'analyzed', 'analyzing'))
test
completion = stemCompletion(test, dictionary = c('analyze', 'analysed','analyzing'))
completion
save.image("C:/Users/junkyu/Desktop/DATA ANALYSIS/모형 실습.RData")
load("C:/Users/junkyu/Desktop/DATA ANALYSIS/모형 실습.RData")
library(tm)
library(KoNLP)
clean_news2
VC.news = VCorpus(VectorSource(clean_news2))
VC.news
summary(VC.news)
VC.news[[1]]$content
TDM_news = TermDocumentMatrix(VC.news)
dim(TDM.news)
dim(TDM_news)
inspect(TDM_news[1:5,])
words = function(doc) {
doc = as.character(doc)
extractNoun(doc)
}
words = function(doc) {
doc = as.character(doc)
extractNoun(doc)
}
TDM_news2 = TermDocumentMatrix(VC.news, control = list(tokenize = words))
dim(TDM_news2)
inspect(TDM_news2)
tdm2 = as.matrix(TDM_news2)
tdm2
tdm3 = rowSums(tdm2)
tdm3
tdm4 = tdm3[order(tfm3, decreasing = T)]
tdm4 = tdm3[order(tdm3, decreasing = T)]
tdm[1:10]
tdm4[1:10]
tdm4[1:10]
mydict = c('빅데이터', '스마트', '산업혁명', '인공지능', '사물인터넷', 'AI', '스타트업', '머신러닝')
my_news = TermDocumentMatrix(VC.news, control=list(tokenize=words, dictionary=mydict))
inspect(my_news)
words = function(doc) {
doc = as.character(doc)
extractNoun(doc)
}
TDM_news2 = TermDocumentMatrix(VC.news, control=list(tokenize=words))
findAssocs(TDM_new2, '빅데이터', 0.9) # 빅데이터란 단어와 무슨 내용이 연관되어 함께 언급되는지 알 수 있음.
findAssocs(TDM_news2, '빅데이터', 0.9) # 빅데이터란 단어와 무슨 내용이 연관되어 함께 언급되는지 알 수 있음.
library(wordcloud)
tdm2 = as.matrix(TDM_news2)
term.freq = sort(rowSums(tdm2), decreasing = T)
head(term.freq)
wordcloud(words = names(term.freq), # term.freq의 이름만 가져옴
freq = term.freq,
min.freq = 5,
random.order = F,
colors = brewer.pal(8, 'Dark2'))
save.image("C:/Users/junkyu/Desktop/DATA ANALYSIS/모형 실습.RData")
a = c(1, 2, 3, 4, 5)
a = c(1, 2, 3, 4, 5) # 벡터
a
sum(a)
sum = c(1, 2, 3, 4, 5)
sum
sum(a)
sum(sum)
25 15*5
2515*5
x^2 - 3*x + 2
x = 1
x^2 - 3*x + 2
A = 100
r = 0.02
n = 10
A*(1+r)^n # 계산이 될까?
25//5
2^^3
1::10
25/5
25/6
25*3/13
print(25*3/13, digits=10)
x <- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)
x
string = c('USA',
'Japan',
'UK',
'France',
'Germany',
'Italy',
'Canada')
c('USA',
'Japan',
'UK',
'France',
'Germany',
'Italy',
'Canada') -> string
string
c('USA',
'Japan',
'UK',
'France',
'Germany',
'Italy',
'Canada') = string
c('USA',
'Japan',
'UK',
'France',
'Germany',
'Italy',
'Canada') -> string
string
string = c('USA',
'Japan',
'UK',
'France',
'Germany',
'Italy',
'Canada',
1)
string
ab = c(1, 2, 3, 4, 5)
a1 = c(1, 2, 3, 4, 5)
_a = c(1, 2, 3, 4, 5)
a_ = c(1, 2, 3, 4, 5)
.1 = c(1, 2, 3, 4, 5)
.1
.01
.a = c(1, 2, 3, 4, 5)
.a
다음 = c(1, 2, 3, 4, 5)
다음
송림 = 1
돌마 = 2
송림
돌마
A = 1
a = 2
print(a)
print(A)
a = 1;
b = 2;
c = 3;
a = 1; b = 2; c = 3;
pi
# 기본 데이터형
pi = 1
pi
# 기본 데이터형
pi = 1
# 기본 데이터형
pi
3^3
T = c(12,3)
T
# 1. 수치형 - 수 0.7,  1/2,
# 2. 논리형 True, False
# 3. 복소수형
# 4. 문자형
mode(3^3)
mode(5>4)
mode(2+5i)
mode('economy')
lotto = read.table('clipboard', sep='\t', header=T)
head(lotto)
library(apriori)
library(arules)
lotto_trans = as(lotto, 'transaction')
lotto_trans = as(lotto, 'transactions')
lotto
library(reshared2)
library(reshape2)
str(lotto)
lot_melt = melt(lotto, id.vars=1)
lot_melt2 = lot_melt[,-2]
lot_melt
str(lot_melt2)
lot_sp = split(lot_melt2$value, lot_melt2$time_id)
lot_ts = as(lop_sp, 'transactions')
lot_ts = as(lot_sp, 'transactions')
lot_ts
inspect(lot_ts)
lotto
lot_melt = melt(lotto, id.vars=1)
lot_melt
lot_sp
lot_melt2
lot_sp
lot_ts
inspect(lot_ts)
# 상위 10개의 로또 번호를 확인
itemFrequencyPlot(lot_ts, topN=10, type='absolute') # 절대도수를 기준으로 파악
itemFrequencyPlot(lot_ts, topN=10) # 상대도수를 기준으로 파악
# 상위 10개의 로또 번호를 확인
itemFrequencyPlot(lot_ts, topN=10, type='absolute') # 절대도수를 기준으로 파악
metric_params = list(supp=0.002, conf=0.8, minlen=2, maxlen=6)
rule_1 = apriori(lot_ts, parameter = metric_params)
inspect(rule_1[1:20,])
# lift를 기준으로 규칙을 내림차순 정렬한 후, 30개의 규칙을 확인
rule_2 = sort(rule_1, by = 'lift', decreasing = T)
rule_2[1:30,]
inspect(rule_2[1:30,])
rule_2 = as.data.frame(rule_2)
rule_2 = as(rule_2, 'data.frame')
rule_2
summary(rule_1)
rule_1
rule_34 = subset(rule_1,rhs %in% '34')
inspect(rule_34)
inspect(rule_most_freq)
rule_most_freq = subset(rule_1,rhs %in% '34')
inspect(rule_most_freq)
load("C:/Users/junkyu/Desktop/DATA ANALYSIS/모형 실습.RData")
lotto = read.table('clipboard', sep='\t', header=T)
lot_melt = melt(lotto, id.vars=1) # 1열을 기준으로 나머지 열을 전부 하나의 열로 맞추기
lot_melt2 = lot_melt[,-2] # 2번째 열은 의미가 없으므로 지우고
str(lot_melt2) # 구조 확인
lot_sp = split(lot_melt2$value, lot_melt2$time_id) # id를 기준으로 구분 짓고 하나로 통합
lot_ts = as(lot_sp, 'transactions') # transactions 데이터로 전환
inspect(lot_ts) # transactions 데이터를 출력하기 위해서 inspect 함수를 사용
# 상위 10개의 로또 번호를 확인
itemFrequencyPlot(lot_ts, topN=10, type='absolute') # 절대도수를 기준으로 파악
itemFrequencyPlot(lot_ts, topN=10) # 상대도수를 기준으로 파악
# 연관분석 수행
metric_params = list(supp=0.002, conf=0.8, minlen=2, maxlen=6) # parameter 생성
rule_1 = apriori(lot_ts, parameter = metric_params) # apriori()를 이용해서 결과 생성, -> 총 679개의 데이터가 생성되었다.
inspect(rule_1[1:20,]) # 해당 규칙 20개만 확인
# lift를 기준으로 규칙을 내림차순 정렬한 후, 30개의 규칙을 확인
rule_2 = sort(rule_1, by = 'lift', decreasing = T)
inspect(rule_2[1:30,])
rule_2 = as(rule_2, 'data.frame')
rule_most_freq = subset(rule_1,rhs %in% '34')
inspect(rule_most_freq)
lot_melt = melt(lotto, id.vars=1) # 1열을 기준으로 나머지 열을 전부 하나의 열로 맞추기
lot_melt
head(lotto)
str(lotto)
str(lot_melt2) # 구조 확인
lot_melt
inspect(lot_ts)
inspect(rule_1[1:20,]) # 해당 규칙 20개만 확인
# 연관분석 수행
metric_params = list(supp=0.002, conf=0.8, minlen=2, maxlen=6) # parameter 생성
rule_1 = apriori(lot_ts, parameter = metric_params) # apriori()를 이용해서 결과 생성, -> 총 679개의 데이터가 생성되었다.
inspect(rule_1[1:20,]) # 해당 규칙 20개만 확인
rule_2 = as(rule_2, 'data.frame')
rule_2
inspect(rule_2[1:30,])
# lift를 기준으로 규칙을 내림차순 정렬한 후, 30개의 규칙을 확인
rule_2 = sort(rule_1, by = 'lift', decreasing = T)
inspect(rule_2[1:30,])
# rule_1의 정보를 summary()함수로 요약
summary(rule_1)
FIFA = read.table('clipboard', sep="\t", header=T)
FIFA = read.table('clipboard', sep=",", header=T)
View(FIFA)
FIFA = read.table('clipboard', sep="\t", header=T)
View(FIFA)
FIFA = read.csv('clipboard')
View(FIFA)
View(FIFA)
View(FIFA)
save.image("C:/Users/junkyu/Desktop/DATA ANALYSIS/모형 실습.RData")
load("C:/Users/junkyu/Desktop/DATA ANALYSIS/모형 실습.RData")
library(tm)
library(rJava)
library(KoNLP)
library(plyr)
useNIADic()
setwd('C:/Users/junkyu/Desktop/DATA ANALYSIS')
buildDictionary(ext_dic = 'woorimalsam',
user_dic = data.frame(readLines('영화 기생충_사전.txt'),
'ncn'),
replace = T)
movie[1:10]
movie = readLines('영화 기생충_review.txt')
dic = readLines('영화 기생충_사전.txt')
movie[1:10]
clean_txt = function(txt) {
txt = tolower(txt) # 소문자로 변환
txt = removePunctuation(txt) # 구두점 제거
txt = removeNumbers(txt) # 숫자 제거
txt = stripWhitespace(txt) # 공백제거
return(txt)
}
movie_clean = clean_txt(movie)
movie_clea[1:10]
movie_clean[1:10]
movie_C = VCorpus(VevtorSource(movie))
movie_C = VCorpus(VectorSource(movie))
clean_corpus= function(corpus) {
corpus = tm_map(corpus, stripWhitespace)
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeNumbers)
corpus = tm_map(corpus, content_transformer)
return(corpus)
}
mov_clean = clean_corpus(movie_C)
dtm = TermDocumentMatrix(mov_clean, control = list(dictionary=dic))
dtm = TermDocumentMatrix(mov_clean, control=list(dictionary=dic))
mov_clean = clean_corpus(movie_C)
mov_clean
m = as.matrix(dtm)
dtm = TermDocumentMatrix(mov_clean, control=list(dictionary=dic))
library(wordcloud)
library(plyr) # 데이터 전처리를 위한 plyr
dtm = TermDocumentMatrix(mov_clean, control=list(dictionary=dic))
useSejongDic()
setwd('C:/Users/junkyu/Desktop/DATA ANALYSIS') # 워킹디렉토리 설정
movie = readLines('영화 기생충_review.txt') # 영화 리뷰 데이터 불러오기
dic = readLines('영화 기생충_사전.txt') # 영화 관련 사전 데이터 불러오기
buildDictionary(ext_dic = 'woorimalsam',
user_dic = data.frame(readLines('영화 기생충_사전.txt'),
'ncn'),
replace = T) # 사전 등록
movie[1:10] # 데이터 확인
# 텍스트 전처리를 위한 함수 작성
clean_txt = function(txt) {
txt = tolower(txt) # 소문자로 변환
txt = removePunctuation(txt) # 구두점 제거
txt = removeNumbers(txt) # 숫자 제거
txt = stripWhitespace(txt) # 공백제거
return(txt)
}
movie_clean = clean_txt(movie)
movie_clean[1:10]
movie_C = VCorpus(VectorSource(movie))
clean_corpus= function(corpus) {
corpus = tm_map(corpus, stripWhitespace)
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeNumbers)
corpus = tm_map(corpus, content_transformer)
return(corpus)
}
mov_clean = clean_corpus(movie_C)
dtm = TermDocumentMatrix(mov_clean, control=list(dictionary=dic))
TermDocumentMatrix
buildDictionary(ext_dic = 'woorimalsam',
user_dic = data.frame(readLines('영화 기생충_사전.txt'),
'ncn'),
replace = T) # 사전 등록
dic = readLines('영화 기생충_사전.txt') # 영화 관련 사전 데이터 불러오기
dic
buildDictionary(ext_dic = 'woorimalsam',
user_dic = data.frame(readLines('영화 기생충_사전.txt'),
'ncn'),
replace = T) # 사전 등록
movie[1:10] # 데이터 확인
buildDictionary(ext_dic = 'woorimalsam',
user_dic = data.frame(readLines('영화 기생충_사전.txt'),
'ncn'),
replace_use_dic = T) # 사전 등록
buildDictionary(ext_dic = 'woorimalsam',
user_dic = data.frame(readLines('영화 기생충_사전.txt'),
'ncn'),
replace_usr_dic = T) # 사전 등록
dtm = TermDocumentMatrix(mov_clean, control=list(dictionary=dic))
movie[1:10] # 데이터 확인
# 텍스트 전처리를 위한 함수 작성
clean_txt = function(txt) {
txt = tolower(txt) # 소문자로 변환
txt = removePunctuation(txt) # 구두점 제거
txt = removeNumbers(txt) # 숫자 제거
txt = stripWhitespace(txt) # 공백제거
return(txt)
}
movie_clean = clean_txt(movie)
movie_clean[1:10]
movie_C = VCorpus(VectorSource(movie))
clean_corpus= function(corpus) {
corpus = tm_map(corpus, stripWhitespace)
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeNumbers)
corpus = tm_map(corpus, content_transformer)
return(corpus)
}
mov_clean = clean_corpus(movie_C)
dtm = TermDocumentMatrix(mov_clean, control=list(dictionary=dic))
movie_C = VCorpus(VectorSource(movie))
clean_corpus= function(corpus) {
corpus = tm_map(corpus, stripWhitespace)
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeNumbers)
corpus = tm_map(corpus, content_transformer(tolower))
return(corpus)
}
mov_clean = clean_corpus(movie_C)
dtm = TermDocumentMatrix(mov_clean, control=list(dictionary=dic))
m = as.matrix(dtm)
v = sort(rowSums(m), decreasing = T)
d = data.frame(word = names(v), freq = v)
head(d, 10)
color = rainbow(nrow(d))
barplot(v[1:10])
legend('right', names(v[1:10]), fill=colors)
colors = rainbow(nrow(d))
barplot(v[1:10])
legend('right', names(v[1:10]), fill=colors)
barplot(v[1:10], main='기생충 review 빈출 명사', col=colors)
legend('right', names(v[1:10]), fill=colors)
barplot(v[1:10], main='기생충 review 빈출 명사', col=colors)
legend("right", names(v[1:10]), fill=colors)
legend("topright", names(v[1:10]), fill=colors)
barplot(v[1:10], main='기생충 review 빈출 명사', col=colors)
legend("topright", names(v[1:10]), fill=colors)
barplot(v[1:10], main='기생충 review 빈출 명사', col=colors)
legend('eastoutside', names(v[1:10]), fill=colors)
legend('eastoutside', names(v[1:10]), fill=colors, cex=1.5)
legend('topright', names(v[1:10]), fill=colors, cex=1.5)
barplot(v[1:10], main='기생충 review 빈출 명사', col=colors)
legend('topright', names(v[1:10]), fill=colors, cex=0.1)
barplot(v[1:10], main='기생충 review 빈출 명사', col=colors)
legend('topright', names(v[1:10]), fill=colors, cex=0.2)
barplot(v[1:10], main='기생충 review 빈출 명사', col=colors)
legend('topright', names(v[1:10]), fill=colors, cex=1)
barplot(v[1:10], main='기생충 review 빈출 명사', col=colors)
legend('topright', names(v[1:10]), fill=colors, cex=0.5)
barplot(v[1:10], main='기생충 review 빈출 명사', col=colors)
legend('topright', names(v[1:10]), fill=colors, cex=0.7)
movie_exN = sapply(movie_clean, extractNoun)
Noun = as.vector(unlist(movie_exN))
Noun_2 = Noun[nchar(Noun) >= 2]
result = data.frame(sort(table(Noun_2),
decreasing = T))
t = wordcloud(result$Noun_2,
result$Freq,
color=brewer.pal(8, 'Dark2'),
min.freq=30)
library(tm) # TermMartix
library(rJava) # KoNLP 패키지를 위한 rjava 불러오기
library(KoNLP) # KoNLP 불러오기
library(wordcloud)
library(plyr) # 데이터 전처리를 위한 plyr
useNIADic() # 사전 로드
useSejongDic()
setwd('C:/Users/junkyu/Desktop/DATA ANALYSIS') # 워킹디렉토리 설정
movie = readLines('영화 기생충_review.txt') # 영화 리뷰 데이터 불러오기
dic = readLines('영화 기생충_사전.txt') # 영화 관련 사전 데이터 불러오기
buildDictionary(ext_dic = 'woorimalsam',
user_dic = data.frame(readLines('영화 기생충_사전.txt'),
'ncn'),
replace_usr_dic = T) # 사전 등록
movie[1:10] # 데이터 확인
# 텍스트 전처리를 위한 함수 작성
clean_txt = function(txt) {
txt = tolower(txt) # 소문자로 변환
txt = removePunctuation(txt) # 구두점 제거
txt = removeNumbers(txt) # 숫자 제거
txt = stripWhitespace(txt) # 공백제거
return(txt)
}
movie_clean = clean_txt(movie)
movie_clean[1:10]
movie_C = VCorpus(VectorSource(movie))
clean_corpus= function(corpus) {
corpus = tm_map(corpus, stripWhitespace)
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeNumbers)
corpus = tm_map(corpus, content_transformer(tolower)) # content_transformer 이거 사용할 때 조심해야 함
return(corpus)
}
mov_clean = clean_corpus(movie_C)
dtm = TermDocumentMatrix(mov_clean, control=list(dictionary=dic))
m = as.matrix(dtm)
v = sort(rowSums(m), decreasing = T)
d = data.frame(word = names(v), freq = v)
head(d, 10)
save.image("C:/Users/junkyu/Desktop/DATA ANALYSIS/모형 실습.RData")
