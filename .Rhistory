return(txt)
}
clean_news = clean_txt(news)
clean_news[[1]]$content
# 숫자, 문장부호, 공백 제거
clean_txt = function(txt) {
txt = tm_map(txt, removeNumbers) # 숫자
txt = tm_map(txt, removePunctuation) # 문장부호
txt = tm_map(txt, stripWhitespace) # 공백
return(txt)
}
clean_news = clean_txt(news)
clean_news[[1]]$content
news_corpus = VCorpus(VectorSource(news))
news_corpus[[1]]$content
# 숫자, 문장부호, 공백 제거
clean_txt = function(txt) {
txt = tm_map(txt, removeNumbers) # 숫자
txt = tm_map(txt, removePunctuation) # 문장부호
txt = tm_map(txt, stripWhitespace) # 공백
return(txt)
}
clean_news = clean_txt(news)
txt = tm_map(txt, removeNumbers)
txt = tm_map(txt, removePunctuation) # 문장부
txt = tm_map(txt, stripWhitespace) # 공백
clean_news = clean_txt(news_corpus)
clean_news[[1]]$content
txt2 = gsub('[[:punct:]]',"",clean_news[[1]])
txt2
install.packages('KoNLP')
library(KoNLP)
install.packages('KoNLP')
library(KoNLP)
# 완벽히 ',·가 지워졌음을 확인
install.packages('rjava')
# 완벽히 ',·가 지워졌음을 확인
install.packages('rJava')
install.packages('KoNLP')
library(KoNLP)
install.packages('KoNLP')
save.image("C:/Users/junkyu/Desktop/DATA ANALYSIS/모형 실습.RData")
install.packages('KoNLP')
# 완벽히 ',·가 지워졌음을 확인
install.packages('rJava')
install.packages('Rccp')
library(KoNLP)
library(KoNLP)
library(rJava)
library(rJava)
library(KoNLP)
useSejongDic()
sentence = '아버지가 방에 스르륵 들어가신다.'
extractNoun(sentence)
load("C:/Users/junkyu/Desktop/DATA ANALYSIS/모형 실습.RData")
sentence = '아버지가 방에 스르륵 들어가신다.'
extractNoun(sentence)
buildDictionary(ext_dic = 'sejong',
user_dic = data.frame(c('스르륵'),c(mag)))
buildDictionary(ext_dic = 'sejong',
user_dic = data.frame(c('스르륵'),c('mag')))
extractNoun(sentence)
SimplePos22(sentence)
clean_txt2 = function(txt) {
txt = removeNumbers(txt)
txt = removePunctuation(txt)
txt = stripWhitespace(txt)
txt = gsub('[^[:alnum:]]','',txt)
return(txt)
}
clean_news2 = clean_txt2(news)
library(tm)
clean_txt2 = function(txt) {
txt = removeNumbers(txt)
txt = removePunctuation(txt)
txt = stripWhitespace(txt)
txt = gsub('[^[:alnum:]]','',txt)
return(txt)
}
clean_news2 = clean_txt2(news)
Noun.news[5]
clean_news2[5]
extractNoun(clean_news2)
Noun.news<-extractNoun(clean.news2)
Noun.news<-extractNoun(clean_news2)
clean_txt2<-function(txt){
txt<-removeNumbers(txt)          #숫자 제거
txt<-removePunctuation(txt)      #문장부호 제거
txt<-stripWhitespace(txt)        #공백제거
txt<-gsub("[^[:alnum:]]"," ",txt) #영숫자, 문자를 제외한 것들을 " "으로 처리
return(txt)
}
clean_news2 = clean_txt2(news)
Noun.news<-extractNoun(clean_news2)
Noun.news[5]
buildDictionary(ext_dic = 'sejong',
user_dic = data.frame(c(read.table('clipboard'))))
extractNoun(clean_news2[5])
install.packages('stringr')
save.image("C:/Users/junkyu/Desktop/DATA ANALYSIS/모형 실습.RData")
install.packages('stringr')
install.packages("stringr")
library(stringr)
library(rJava)
library(KoNLP)
library(tm)
doc1 = paste(SimplePos22(clean_news2[[2]]))
library(stringr)
doc1 = paste(SimplePos22(clean_news2[[2]]))
doc1
doc2 = str_match(doc1, '([가-힣]+)/PA')
doc2
doc3 = doc2[,2]
doc[!is.na(doc3)]
doc3[!is.na(doc3)]
test = stemDocument(c('analyze', 'analyzed', 'analyzing'))
test
completion = stemCompletion(test, dictionary = c('analyze', 'analysed','analyzing'))
completion
save.image("C:/Users/junkyu/Desktop/DATA ANALYSIS/모형 실습.RData")
load("C:/Users/junkyu/Desktop/DATA ANALYSIS/모형 실습.RData")
library(tm)
library(KoNLP)
clean_news2
VC.news = VCorpus(VectorSource(clean_news2))
VC.news
summary(VC.news)
VC.news[[1]]$content
TDM_news = TermDocumentMatrix(VC.news)
dim(TDM.news)
dim(TDM_news)
inspect(TDM_news[1:5,])
words = function(doc) {
doc = as.character(doc)
extractNoun(doc)
}
words = function(doc) {
doc = as.character(doc)
extractNoun(doc)
}
TDM_news2 = TermDocumentMatrix(VC.news, control = list(tokenize = words))
dim(TDM_news2)
inspect(TDM_news2)
tdm2 = as.matrix(TDM_news2)
tdm2
tdm3 = rowSums(tdm2)
tdm3
tdm4 = tdm3[order(tfm3, decreasing = T)]
tdm4 = tdm3[order(tdm3, decreasing = T)]
tdm[1:10]
tdm4[1:10]
tdm4[1:10]
mydict = c('빅데이터', '스마트', '산업혁명', '인공지능', '사물인터넷', 'AI', '스타트업', '머신러닝')
my_news = TermDocumentMatrix(VC.news, control=list(tokenize=words, dictionary=mydict))
inspect(my_news)
words = function(doc) {
doc = as.character(doc)
extractNoun(doc)
}
TDM_news2 = TermDocumentMatrix(VC.news, control=list(tokenize=words))
findAssocs(TDM_new2, '빅데이터', 0.9) # 빅데이터란 단어와 무슨 내용이 연관되어 함께 언급되는지 알 수 있음.
findAssocs(TDM_news2, '빅데이터', 0.9) # 빅데이터란 단어와 무슨 내용이 연관되어 함께 언급되는지 알 수 있음.
library(wordcloud)
tdm2 = as.matrix(TDM_news2)
term.freq = sort(rowSums(tdm2), decreasing = T)
head(term.freq)
wordcloud(words = names(term.freq), # term.freq의 이름만 가져옴
freq = term.freq,
min.freq = 5,
random.order = F,
colors = brewer.pal(8, 'Dark2'))
save.image("C:/Users/junkyu/Desktop/DATA ANALYSIS/모형 실습.RData")
a = c(1, 2, 3, 4, 5)
a = c(1, 2, 3, 4, 5) # 벡터
a
sum(a)
sum = c(1, 2, 3, 4, 5)
sum
sum(a)
sum(sum)
25 15*5
2515*5
x^2 - 3*x + 2
x = 1
x^2 - 3*x + 2
A = 100
r = 0.02
n = 10
A*(1+r)^n # 계산이 될까?
25//5
2^^3
1::10
25/5
25/6
25*3/13
print(25*3/13, digits=10)
x <- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)
x
string = c('USA',
'Japan',
'UK',
'France',
'Germany',
'Italy',
'Canada')
c('USA',
'Japan',
'UK',
'France',
'Germany',
'Italy',
'Canada') -> string
string
c('USA',
'Japan',
'UK',
'France',
'Germany',
'Italy',
'Canada') = string
c('USA',
'Japan',
'UK',
'France',
'Germany',
'Italy',
'Canada') -> string
string
string = c('USA',
'Japan',
'UK',
'France',
'Germany',
'Italy',
'Canada',
1)
string
ab = c(1, 2, 3, 4, 5)
a1 = c(1, 2, 3, 4, 5)
_a = c(1, 2, 3, 4, 5)
a_ = c(1, 2, 3, 4, 5)
.1 = c(1, 2, 3, 4, 5)
.1
.01
.a = c(1, 2, 3, 4, 5)
.a
다음 = c(1, 2, 3, 4, 5)
다음
송림 = 1
돌마 = 2
송림
돌마
A = 1
a = 2
print(a)
print(A)
a = 1;
b = 2;
c = 3;
a = 1; b = 2; c = 3;
pi
# 기본 데이터형
pi = 1
pi
# 기본 데이터형
pi = 1
# 기본 데이터형
pi
3^3
T = c(12,3)
T
# 1. 수치형 - 수 0.7,  1/2,
# 2. 논리형 True, False
# 3. 복소수형
# 4. 문자형
mode(3^3)
mode(5>4)
mode(2+5i)
mode('economy')
lotto = read.table('clipboard', sep='\t', header=T)
head(lotto)
library(apriori)
library(arules)
lotto_trans = as(lotto, 'transaction')
lotto_trans = as(lotto, 'transactions')
lotto
library(reshared2)
library(reshape2)
str(lotto)
lot_melt = melt(lotto, id.vars=1)
lot_melt2 = lot_melt[,-2]
lot_melt
str(lot_melt2)
lot_sp = split(lot_melt2$value, lot_melt2$time_id)
lot_ts = as(lop_sp, 'transactions')
lot_ts = as(lot_sp, 'transactions')
lot_ts
inspect(lot_ts)
lotto
lot_melt = melt(lotto, id.vars=1)
lot_melt
lot_sp
lot_melt2
lot_sp
lot_ts
inspect(lot_ts)
# 상위 10개의 로또 번호를 확인
itemFrequencyPlot(lot_ts, topN=10, type='absolute') # 절대도수를 기준으로 파악
itemFrequencyPlot(lot_ts, topN=10) # 상대도수를 기준으로 파악
# 상위 10개의 로또 번호를 확인
itemFrequencyPlot(lot_ts, topN=10, type='absolute') # 절대도수를 기준으로 파악
metric_params = list(supp=0.002, conf=0.8, minlen=2, maxlen=6)
rule_1 = apriori(lot_ts, parameter = metric_params)
inspect(rule_1[1:20,])
# lift를 기준으로 규칙을 내림차순 정렬한 후, 30개의 규칙을 확인
rule_2 = sort(rule_1, by = 'lift', decreasing = T)
rule_2[1:30,]
inspect(rule_2[1:30,])
rule_2 = as.data.frame(rule_2)
rule_2 = as(rule_2, 'data.frame')
rule_2
summary(rule_1)
rule_1
rule_34 = subset(rule_1,rhs %in% '34')
inspect(rule_34)
inspect(rule_most_freq)
rule_most_freq = subset(rule_1,rhs %in% '34')
inspect(rule_most_freq)
load("C:/Users/junkyu/Desktop/DATA ANALYSIS/모형 실습.RData")
lotto = read.table('clipboard', sep='\t', header=T)
lot_melt = melt(lotto, id.vars=1) # 1열을 기준으로 나머지 열을 전부 하나의 열로 맞추기
lot_melt2 = lot_melt[,-2] # 2번째 열은 의미가 없으므로 지우고
str(lot_melt2) # 구조 확인
lot_sp = split(lot_melt2$value, lot_melt2$time_id) # id를 기준으로 구분 짓고 하나로 통합
lot_ts = as(lot_sp, 'transactions') # transactions 데이터로 전환
inspect(lot_ts) # transactions 데이터를 출력하기 위해서 inspect 함수를 사용
# 상위 10개의 로또 번호를 확인
itemFrequencyPlot(lot_ts, topN=10, type='absolute') # 절대도수를 기준으로 파악
itemFrequencyPlot(lot_ts, topN=10) # 상대도수를 기준으로 파악
# 연관분석 수행
metric_params = list(supp=0.002, conf=0.8, minlen=2, maxlen=6) # parameter 생성
rule_1 = apriori(lot_ts, parameter = metric_params) # apriori()를 이용해서 결과 생성, -> 총 679개의 데이터가 생성되었다.
inspect(rule_1[1:20,]) # 해당 규칙 20개만 확인
# lift를 기준으로 규칙을 내림차순 정렬한 후, 30개의 규칙을 확인
rule_2 = sort(rule_1, by = 'lift', decreasing = T)
inspect(rule_2[1:30,])
rule_2 = as(rule_2, 'data.frame')
rule_most_freq = subset(rule_1,rhs %in% '34')
inspect(rule_most_freq)
lot_melt = melt(lotto, id.vars=1) # 1열을 기준으로 나머지 열을 전부 하나의 열로 맞추기
lot_melt
head(lotto)
str(lotto)
str(lot_melt2) # 구조 확인
lot_melt
inspect(lot_ts)
inspect(rule_1[1:20,]) # 해당 규칙 20개만 확인
# 연관분석 수행
metric_params = list(supp=0.002, conf=0.8, minlen=2, maxlen=6) # parameter 생성
rule_1 = apriori(lot_ts, parameter = metric_params) # apriori()를 이용해서 결과 생성, -> 총 679개의 데이터가 생성되었다.
inspect(rule_1[1:20,]) # 해당 규칙 20개만 확인
rule_2 = as(rule_2, 'data.frame')
rule_2
inspect(rule_2[1:30,])
# lift를 기준으로 규칙을 내림차순 정렬한 후, 30개의 규칙을 확인
rule_2 = sort(rule_1, by = 'lift', decreasing = T)
inspect(rule_2[1:30,])
# rule_1의 정보를 summary()함수로 요약
summary(rule_1)
FIFA = read.table('clipboard', sep="\t", header=T)
FIFA = read.table('clipboard', sep=",", header=T)
View(FIFA)
FIFA = read.table('clipboard', sep="\t", header=T)
View(FIFA)
FIFA = read.csv('clipboard')
View(FIFA)
View(FIFA)
View(FIFA)
save.image("C:/Users/junkyu/Desktop/DATA ANALYSIS/모형 실습.RData")
vector1 = c(1, 2, 3)
vector1
mode(vector1)
mode(vector1)
vector2 = c(T, F, T, T)
mode(vector2)
vector3 = c(1+1i, 1+4i, 4+1i, 5+9i)
mode(vector3)
vector4 = c('a', 'b', 'c')
mode(vector4)
vector2 = c(1, 0, 1, 1)
mode(vector2) # 논리형 벡터
vector5 = c(1, T, 'a', 1+1i)
vector5
vector5 = c(1, T, 1+1i)
vector5
vector5 = c(1, T)
vector5
vector5 = c(3, T)
vector5
mode(vector5)
vector5 = c(3, T, 'a')
vector5
mode(vector5)
vector5 = c(1, 2, 3)
vector6 = c(3, 4)
vector5 = c(1, 2, 3)
vector6 = c(4, 5, 6)
vector5 + vector6
vector5 * vector6
vector5 / vector6
%
vector5 % vector6
vector5 /%/ vector6
vector5 %/% vector6
vector5 %% vector6
vector5 = c(1, 2, 3)
vector6 = c(4, 5)
vector5 + vector6
vector5 = c(1, 2, 3, 4)
vector6 = c(4, 5)
vector5 + vector6
vector5 - vector6
vector5 * vector6
vector5 / vector6
vector5 %% vector6
vector5 %/% vector6
as.character(c(1,2,3,4))
mode(as.character(c(1,2,3,4)))
exp(500)
exp(1000000000)
1/0
a = c()
a
x = c(1, 2, 3, NA, 5)
x
y = c(2, 4, 5, 8, 10)
y
x + y
0/0
Inf/1
c()
is.na(x)
x[2] <- NaN
x
is.nan(x)
length(x)
x = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)
1:100
x = 1:100
mode(x)
x = c(1:100)
mode(x)
x
-1:-100
-1.1:-100.1
load("C:/Users/junkyu/Desktop/DATA ANALYSIS/ADP 모의고사2회.RData")
setwd('C:/Users/junkyu/Desktop/DATA ANALYSIS')
keynote = readLines('연설문.txt')
keynote
library(rJava)
library(tm)
library(KoNLP)
library(dplyr)
library(plyr)
library(wordcloud)
useSejongDic()
clean_dic = function(txt) {
txt = tolower(txt)
txt = removePunctuation(txt)
txt = removeNumbers(txt)
txt = stripWhitespace(txt)
return(txt)
}
moon_1 = clean_txt(moon)
clean_txt = function(txt) {
txt = tolower(txt)
txt = removePunctuation(txt)
txt = removeNumbers(txt)
txt = stripWhitespace(txt)
return(txt)
}
moon_1 = clean_txt(moon)
keynote_1 = clean_txt(keynote)
keynote_1 = clean_txt(keynote)
noun = extractNoun(keynote_1)
wordcnt = table(unlist(keynote_1))
keynote_1 = clean_txt(keynote)
noun = extractNoun(keynote_1)
wordcnt = table(unlist(keynote_1))
cnoun = as.data.frame(wordcnt, stringsAsFactors = F)
table.cnoun = cnoun[nchar(cnoun$Var1) >= 2,]
table.cnoun
keynote_1 = clean_txt(keynote)
noun = extractNoun(keynote_1)
wordcnt = table(unlist(noun))
cnoun = as.data.frame(wordcnt, stringsAsFactors = F)
table.cnoun = cnoun[nchar(cnoun$Var1) >= 2,]
table.cnoun
top_10 = table.cnoun %>% arrange(-Freq) %>%  head(10)
top_10
barplot(top_10$Freq,
names=top_10$Var1,
main='문재인 대통령 취임사 빈출 명사',
xlab='단어',
ylab='빈도')
result = table.cnoun %>%  arrange(-Freq)
result = table.cnoun %>%  arrange(-Freq)
t = wordcloud(result$Var1,
result$Freq,
color=brewer.pal(6, 'Dark2'),
min.freq = 2)
save.image("C:/Users/junkyu/Desktop/DATA ANALYSIS/ADP 모의고사2회.RData")
